rollout_type: ofa

## ---- Component search_space ----
# ---- Type ofa ----
search_space_type: ofa
search_space_cfg:
  num_cell_groups: [1, 4, 4, 4, 4, 4, 1]
  expansions: [1, 6, 6, 6, 6, 6, 6]
  image_size_choice: [32]
  schedule_cfg:
    kernel_choice:
        type: value
        boundary: [1, 51, 120]
        value: [[3], [3], [3]]
    width_choice:
      type: value
      boundary: [1, 180, 240]
      value: [[6], [6, 5, 4], [6, 5, 4, 3, 2]]
    depth_choice:
      type: value
      boundary: [1, 300, 360]
      value: [[4], [4, 3], [4,3,2]]
# ---- End Type ofa ----
## ---- End Component search_space ----

## ---- Component dataset ----
# ---- Type cifar10 ----
dataset_type: cifar10
dataset_cfg:
  # Schedulable attributes:
  cutout: null
# ---- End Type cifar10 ----
## ---- End Component dataset ----

objective_type: ofa_classification
objective_cfg:
  is_finetune: True
  # Schedulable attributes:
  schedule_cfg:
    soft_loss_coeff:
      type: value
      boundary: [1, 60]
      value: [0.0, 1.0]
  soft_loss_coeff: 0.0
  latency_coeff: 1.0

final_model_type: ofa_final_model
final_model_cfg:
  backbone_type: mbv2_backbone
  genotypes: "image_size=32, cell_0=1, cell_1=2, cell_2=2, cell_3=3, cell_4=4, cell_5=3, cell_6=1, cell_0_block_0=(1, 3), cell_1_block_0=(1, 3), cell_1_block_1=(4, 3), cell_1_block_2=(3, 5), cell_1_block_3=(2, 3), cell_2_block_0=(3, 3), cell_2_block_1=(3, 5), cell_2_block_2=(6, 3), cell_2_block_3=(5, 3), cell_3_block_0=(3, 5), cell_3_block_1=(3, 5), cell_3_block_2=(6, 3), cell_3_block_3=(3, 3), cell_4_block_0=(3, 3), cell_4_block_1=(2, 3), cell_4_block_2=(2, 3), cell_4_block_3=(6, 3), cell_5_block_0=(6, 3), cell_5_block_1=(6, 5), cell_5_block_2=(4, 5), cell_5_block_3=(4, 5), cell_6_block_0=(6, 3)"

  backbone_cfg:
    mult_ratio: 1.0
    layer_channels: [32, 16, 24, 40, 80, 112, 160, 960, 1280]
    strides: [1, 2, 1, 2, 1, 1, 1]
    kernel_sizes: [3, 5, 7]
    stem_stride: 1
  # supernet_state_dict: "results/search_cifar10/final/evaluator.pt"

final_trainer_type: cnn_trainer
final_trainer_cfg:
  batch_size: 256
  epochs: 100
  optimizer_scheduler:
    eta_min: 0.00001
    T_max: 100
    type: CosineAnnealingLR
  schedule_cfg: null
  weight_decay: 0.0003
  learning_rate: 0.1
  save_as_state_dict: true
